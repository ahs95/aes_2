{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T16:12:28.168544Z","iopub.execute_input":"2024-06-12T16:12:28.169059Z","iopub.status.idle":"2024-06-12T16:12:28.542424Z","shell.execute_reply.started":"2024-06-12T16:12:28.169023Z","shell.execute_reply":"2024-06-12T16:12:28.541547Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AdamW, Trainer, TrainingArguments\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\nfrom sklearn.metrics import cohen_kappa_score","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:12:28.544099Z","iopub.execute_input":"2024-06-12T16:12:28.544478Z","iopub.status.idle":"2024-06-12T16:12:36.201893Z","shell.execute_reply.started":"2024-06-12T16:12:28.544451Z","shell.execute_reply":"2024-06-12T16:12:36.201136Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-12 16:12:33.849963: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-12 16:12:33.850020: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-12 16:12:33.851465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:12:36.202959Z","iopub.execute_input":"2024-06-12T16:12:36.203485Z","iopub.status.idle":"2024-06-12T16:12:36.575063Z","shell.execute_reply.started":"2024-06-12T16:12:36.203457Z","shell.execute_reply":"2024-06-12T16:12:36.574159Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      essay_id                                          full_text  score\n0      000d118  Many people have car where they live. The thin...      3\n1      000fe60  I am a scientist at NASA that is discussing th...      3\n2      001ab80  People always wish they had the same technolog...      4\n3      001bdc0  We all heard about Venus, the planet without a...      4\n4      002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3\n...        ...                                                ...    ...\n17302  ffd378d  the story \" The Challenge of Exploing Venus \" ...      2\n17303  ffddf1f  Technology has changed a lot of ways that we l...      4\n17304  fff016d  If you don't like sitting around all day than ...      2\n17305  fffb49b  In \"The Challenge of Exporing Venus,\" the auth...      1\n17306  fffed3e  Venus is worthy place to study but dangerous. ...      2\n\n[17307 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>full_text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d118</td>\n      <td>Many people have car where they live. The thin...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fe60</td>\n      <td>I am a scientist at NASA that is discussing th...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ab80</td>\n      <td>People always wish they had the same technolog...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001bdc0</td>\n      <td>We all heard about Venus, the planet without a...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>002ba53</td>\n      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17302</th>\n      <td>ffd378d</td>\n      <td>the story \" The Challenge of Exploing Venus \" ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17303</th>\n      <td>ffddf1f</td>\n      <td>Technology has changed a lot of ways that we l...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>17304</th>\n      <td>fff016d</td>\n      <td>If you don't like sitting around all day than ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17305</th>\n      <td>fffb49b</td>\n      <td>In \"The Challenge of Exporing Venus,\" the auth...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17306</th>\n      <td>fffed3e</td>\n      <td>Venus is worthy place to study but dangerous. ...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>17307 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Remove essay_id column\ndata.drop('essay_id', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:12:36.577266Z","iopub.execute_input":"2024-06-12T16:12:36.577547Z","iopub.status.idle":"2024-06-12T16:12:36.583398Z","shell.execute_reply.started":"2024-06-12T16:12:36.577522Z","shell.execute_reply":"2024-06-12T16:12:36.582406Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:12:36.584494Z","iopub.execute_input":"2024-06-12T16:12:36.584736Z","iopub.status.idle":"2024-06-12T16:12:36.907787Z","shell.execute_reply.started":"2024-06-12T16:12:36.584715Z","shell.execute_reply":"2024-06-12T16:12:36.906952Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"import zipfile\n\n# Unzip wordnet.zip\nzip_file_path = '/usr/share/nltk_data/corpora/wordnet.zip'\nextract_to_path = '/usr/share/nltk_data/corpora/'\n\nwith zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_to_path)\n\nprint(\"Unzipping completed successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:12:36.909008Z","iopub.execute_input":"2024-06-12T16:12:36.909324Z","iopub.status.idle":"2024-06-12T16:12:37.189784Z","shell.execute_reply.started":"2024-06-12T16:12:36.909295Z","shell.execute_reply":"2024-06-12T16:12:37.188806Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Unzipping completed successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_text(text):\n    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]  # Filter out non-alphabetic tokens and stopwords\n    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n    return ' '.join(lemmatized_tokens)\n\n# Apply preprocessing to the 'full_text' column\ndata['full_text'] = data['full_text'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:12:37.191529Z","iopub.execute_input":"2024-06-12T16:12:37.191824Z","iopub.status.idle":"2024-06-12T16:14:16.268763Z","shell.execute_reply.started":"2024-06-12T16:12:37.191798Z","shell.execute_reply":"2024-06-12T16:14:16.267957Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:16.270203Z","iopub.execute_input":"2024-06-12T16:14:16.270469Z","iopub.status.idle":"2024-06-12T16:14:17.620607Z","shell.execute_reply.started":"2024-06-12T16:14:16.270447Z","shell.execute_reply":"2024-06-12T16:14:17.619271Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModel\nmodel = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-xsmall\",num_labels=6)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:17.621784Z","iopub.execute_input":"2024-06-12T16:14:17.622107Z","iopub.status.idle":"2024-06-12T16:14:17.974412Z","shell.execute_reply.started":"2024-06-12T16:14:17.622080Z","shell.execute_reply":"2024-06-12T16:14:17.973676Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenize and encode the data\ndef tokenize_data(text_list):\n    return tokenizer(text_list, truncation=True, padding=True, max_length=1024, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:17.977325Z","iopub.execute_input":"2024-06-12T16:14:17.977597Z","iopub.status.idle":"2024-06-12T16:14:17.981936Z","shell.execute_reply.started":"2024-06-12T16:14:17.977572Z","shell.execute_reply":"2024-06-12T16:14:17.981046Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Split data into training and testing sets\ntrain_texts, test_texts, y_train, y_test = train_test_split(data['full_text'], data['score'], test_size=0.2, random_state=42, stratify = data.score)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:17.982955Z","iopub.execute_input":"2024-06-12T16:14:17.983235Z","iopub.status.idle":"2024-06-12T16:14:18.005068Z","shell.execute_reply.started":"2024-06-12T16:14:17.983211Z","shell.execute_reply":"2024-06-12T16:14:18.004362Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Tokenize the texts\ntrain_encodings = tokenize_data(train_texts.tolist())\ntest_encodings = tokenize_data(test_texts.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:18.005887Z","iopub.execute_input":"2024-06-12T16:14:18.006152Z","iopub.status.idle":"2024-06-12T16:14:37.715716Z","shell.execute_reply.started":"2024-06-12T16:14:18.006129Z","shell.execute_reply":"2024-06-12T16:14:37.714698Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Verify and adjust label values\ntrain_labels = torch.tensor(y_train.values)\ntest_labels = torch.tensor(y_test.values)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:37.717373Z","iopub.execute_input":"2024-06-12T16:14:37.717760Z","iopub.status.idle":"2024-06-12T16:14:37.722821Z","shell.execute_reply.started":"2024-06-12T16:14:37.717723Z","shell.execute_reply":"2024-06-12T16:14:37.721950Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# If labels are out of range, apply necessary adjustments (if needed)\nnum_labels = 6\ntrain_labels = torch.clamp(train_labels, 0, num_labels - 1)\ntest_labels = torch.clamp(test_labels, 0, num_labels - 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:37.724006Z","iopub.execute_input":"2024-06-12T16:14:37.724329Z","iopub.status.idle":"2024-06-12T16:14:37.734941Z","shell.execute_reply.started":"2024-06-12T16:14:37.724297Z","shell.execute_reply":"2024-06-12T16:14:37.734085Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Custom Dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:37.736110Z","iopub.execute_input":"2024-06-12T16:14:37.736810Z","iopub.status.idle":"2024-06-12T16:14:37.748149Z","shell.execute_reply.started":"2024-06-12T16:14:37.736778Z","shell.execute_reply":"2024-06-12T16:14:37.747347Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Create Dataset objects\ntrain_dataset = CustomDataset(train_encodings, train_labels)\ntest_dataset = CustomDataset(test_encodings, test_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:37.749178Z","iopub.execute_input":"2024-06-12T16:14:37.749438Z","iopub.status.idle":"2024-06-12T16:14:37.763531Z","shell.execute_reply.started":"2024-06-12T16:14:37.749415Z","shell.execute_reply":"2024-06-12T16:14:37.762750Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Custom Trainer class for OLL\nclass OLL2Trainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        num_classes = model.config.num_labels\n        dist_matrix = torch.arange(num_classes).unsqueeze(0) - torch.arange(num_classes).unsqueeze(1)\n        dist_matrix = dist_matrix ** 2  # Square distances\n        dist_matrix = dist_matrix.float().to(model.device)\n        \n        labels = inputs[\"labels\"]\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probas = F.softmax(logits, dim=1)\n        true_labels = [num_classes * [labels[k].item()] for k in range(len(labels))]\n        label_ids = len(labels) * [[k for k in range(num_classes)]]\n        distances = [[float(dist_matrix[true_labels[j][i]][label_ids[j][i]]) for i in range(num_classes)] for j in range(len(labels))]\n        distances_tensor = torch.tensor(distances, device=model.device, requires_grad=True)\n        err = -torch.log(1 - probas) * abs(distances_tensor)\n        loss = torch.sum(err, axis=1).mean()\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:37.764475Z","iopub.execute_input":"2024-06-12T16:14:37.764704Z","iopub.status.idle":"2024-06-12T16:14:37.775772Z","shell.execute_reply.started":"2024-06-12T16:14:37.764683Z","shell.execute_reply":"2024-06-12T16:14:37.774926Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/trained_deberta_model',\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    optim=\"adamw_bnb_8bit\",\n    learning_rate = 1e-3, \n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"loss\",\n    report_to=\"none\"  # Disable default logging to W&B\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:37.776818Z","iopub.execute_input":"2024-06-12T16:14:37.777146Z","iopub.status.idle":"2024-06-12T16:14:37.859298Z","shell.execute_reply.started":"2024-06-12T16:14:37.777123Z","shell.execute_reply":"2024-06-12T16:14:37.858274Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Trainer instance\ntrainer = OLL2Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:37.860363Z","iopub.execute_input":"2024-06-12T16:14:37.860640Z","iopub.status.idle":"2024-06-12T16:14:38.042179Z","shell.execute_reply.started":"2024-06-12T16:14:37.860616Z","shell.execute_reply":"2024-06-12T16:14:38.041252Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train and evaluate the model, logging train and validation loss\ndef train_and_evaluate(trainer):\n    for epoch in range(int(training_args.num_train_epochs)):\n        train_result = trainer.train()\n        eval_result = trainer.evaluate()\n        train_loss = train_result.training_loss\n        eval_loss = eval_result[\"eval_loss\"]\n        print(f\"Epoch {epoch + 1}: Training Loss: {train_loss}, Validation Loss: {eval_loss}\")\n\ntrain_and_evaluate(trainer)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:14:38.043241Z","iopub.execute_input":"2024-06-12T16:14:38.043518Z","iopub.status.idle":"2024-06-12T18:04:11.442527Z","shell.execute_reply.started":"2024-06-12T16:14:38.043493Z","shell.execute_reply":"2024-06-12T18:04:11.441595Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3462' max='3462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3462/3462 53:34, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.896400</td>\n      <td>1.887321</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.900300</td>\n      <td>1.870461</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1732' max='866' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [866/866 27:58]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Epoch 1: Training Loss: 1.9033099678057182, Validation Loss: 1.8704606294631958\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3462' max='3462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3462/3462 53:30, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.905700</td>\n      <td>1.877758</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.908800</td>\n      <td>1.870273</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='866' max='866' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [866/866 01:11]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Epoch 2: Training Loss: 1.8751097708888855, Validation Loss: 1.8702733516693115\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to compute QWK score\ndef compute_qwk(true_labels, pred_labels):\n    qwk_score = cohen_kappa_score(true_labels, pred_labels, weights='quadratic')\n    return qwk_score\n\n# Get predictions on the validation set\ndef evaluate_model(trainer, eval_dataloader):\n    all_preds = []\n    all_labels = []\n    for batch in eval_dataloader:\n        batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = trainer.model(**batch)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=-1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(batch[\"labels\"].cpu().numpy())\n    return all_labels, all_preds\n\n# Compute and print QWK score\neval_dataloader = trainer.get_eval_dataloader()\ntrue_labels, pred_labels = evaluate_model(trainer, eval_dataloader)\nqwk_score = compute_qwk(true_labels, pred_labels)\nprint(f\"Quadratic Weighted Kappa score: {qwk_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T18:15:11.217495Z","iopub.execute_input":"2024-06-12T18:15:11.217878Z","iopub.status.idle":"2024-06-12T18:16:21.533446Z","shell.execute_reply.started":"2024-06-12T18:15:11.217843Z","shell.execute_reply":"2024-06-12T18:16:21.532542Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Quadratic Weighted Kappa score: 0.0\n","output_type":"stream"}]}]}