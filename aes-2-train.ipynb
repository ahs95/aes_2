{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/arshadulhoque/aes-2-train?scriptVersionId=181487554\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-04T15:06:25.064382Z","iopub.execute_input":"2024-06-04T15:06:25.065154Z","iopub.status.idle":"2024-06-04T15:06:25.444529Z","shell.execute_reply.started":"2024-06-04T15:06:25.065112Z","shell.execute_reply":"2024-06-04T15:06:25.443543Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AdamW\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\nfrom sklearn.metrics import cohen_kappa_score, classification_report, confusion_matrix, f1_score","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:06:25.446647Z","iopub.execute_input":"2024-06-04T15:06:25.447489Z","iopub.status.idle":"2024-06-04T15:06:43.431895Z","shell.execute_reply.started":"2024-06-04T15:06:25.447452Z","shell.execute_reply":"2024-06-04T15:06:43.430959Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-04 15:06:34.645755: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-04 15:06:34.645851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-04 15:06:34.788366: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:06:43.433071Z","iopub.execute_input":"2024-06-04T15:06:43.433637Z","iopub.status.idle":"2024-06-04T15:06:44.214242Z","shell.execute_reply.started":"2024-06-04T15:06:43.43361Z","shell.execute_reply":"2024-06-04T15:06:44.21327Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      essay_id                                          full_text  score\n0      000d118  Many people have car where they live. The thin...      3\n1      000fe60  I am a scientist at NASA that is discussing th...      3\n2      001ab80  People always wish they had the same technolog...      4\n3      001bdc0  We all heard about Venus, the planet without a...      4\n4      002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3\n...        ...                                                ...    ...\n17302  ffd378d  the story \" The Challenge of Exploing Venus \" ...      2\n17303  ffddf1f  Technology has changed a lot of ways that we l...      4\n17304  fff016d  If you don't like sitting around all day than ...      2\n17305  fffb49b  In \"The Challenge of Exporing Venus,\" the auth...      1\n17306  fffed3e  Venus is worthy place to study but dangerous. ...      2\n\n[17307 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>full_text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d118</td>\n      <td>Many people have car where they live. The thin...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fe60</td>\n      <td>I am a scientist at NASA that is discussing th...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ab80</td>\n      <td>People always wish they had the same technolog...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001bdc0</td>\n      <td>We all heard about Venus, the planet without a...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>002ba53</td>\n      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17302</th>\n      <td>ffd378d</td>\n      <td>the story \" The Challenge of Exploing Venus \" ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17303</th>\n      <td>ffddf1f</td>\n      <td>Technology has changed a lot of ways that we l...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>17304</th>\n      <td>fff016d</td>\n      <td>If you don't like sitting around all day than ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17305</th>\n      <td>fffb49b</td>\n      <td>In \"The Challenge of Exporing Venus,\" the auth...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17306</th>\n      <td>fffed3e</td>\n      <td>Venus is worthy place to study but dangerous. ...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>17307 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Remove essay_id column\ndata.drop('essay_id', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:06:44.217165Z","iopub.execute_input":"2024-06-04T15:06:44.217638Z","iopub.status.idle":"2024-06-04T15:06:44.22552Z","shell.execute_reply.started":"2024-06-04T15:06:44.217613Z","shell.execute_reply":"2024-06-04T15:06:44.22454Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(data, test_size=0.2, stratify=data['score'], random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:06:44.226668Z","iopub.execute_input":"2024-06-04T15:06:44.226979Z","iopub.status.idle":"2024-06-04T15:06:44.247058Z","shell.execute_reply.started":"2024-06-04T15:06:44.22695Z","shell.execute_reply":"2024-06-04T15:06:44.246381Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:06:44.248018Z","iopub.execute_input":"2024-06-04T15:06:44.24827Z","iopub.status.idle":"2024-06-04T15:06:46.572985Z","shell.execute_reply.started":"2024-06-04T15:06:44.248248Z","shell.execute_reply":"2024-06-04T15:06:46.571991Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8822207fcbeb4f3f90f9ef19df9b2ef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b1c43dae7f74b5f9c2f857a8966a4c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5abd0ac7a4a546cfbe671d2219c7fa97"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoModel\nmodel = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-xsmall\", num_labels=6)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:06:46.574176Z","iopub.execute_input":"2024-06-04T15:06:46.574511Z","iopub.status.idle":"2024-06-04T15:06:48.161839Z","shell.execute_reply.started":"2024-06-04T15:06:46.574485Z","shell.execute_reply":"2024-06-04T15:06:48.161019Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4af9e8b8e7b34bed9fc7f0c14f47e308"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenize and encode the training data\ntrain_encodings = tokenizer(list(train_data['full_text']), truncation=True, padding=True, max_length=1024, return_tensors='pt')\ntrain_labels = torch.tensor(train_data['score'].values - 1)  # Adjust labels to start from 0","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:06:48.163138Z","iopub.execute_input":"2024-06-04T15:06:48.163587Z","iopub.status.idle":"2024-06-04T15:07:09.558166Z","shell.execute_reply.started":"2024-06-04T15:06:48.163553Z","shell.execute_reply":"2024-06-04T15:07:09.557348Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Tokenize and encode the testing data\ntest_encodings = tokenizer(list(test_data['full_text']), truncation=True, padding=True, max_length=1024, return_tensors='pt')\ntest_labels = torch.tensor(test_data['score'].values - 1)  # Adjust labels to start from 0","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:07:09.559518Z","iopub.execute_input":"2024-06-04T15:07:09.55988Z","iopub.status.idle":"2024-06-04T15:07:14.691912Z","shell.execute_reply.started":"2024-06-04T15:07:09.559845Z","shell.execute_reply":"2024-06-04T15:07:14.69106Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Create DataLoader for training and testing sets\ntrain_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\ntest_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:07:14.694724Z","iopub.execute_input":"2024-06-04T15:07:14.695028Z","iopub.status.idle":"2024-06-04T15:07:14.701097Z","shell.execute_reply.started":"2024-06-04T15:07:14.695003Z","shell.execute_reply":"2024-06-04T15:07:14.700105Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:07:14.702567Z","iopub.execute_input":"2024-06-04T15:07:14.702954Z","iopub.status.idle":"2024-06-04T15:07:14.751092Z","shell.execute_reply.started":"2024-06-04T15:07:14.702922Z","shell.execute_reply":"2024-06-04T15:07:14.749772Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Move model to appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  model = nn.DataParallel(model)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:07:14.754882Z","iopub.execute_input":"2024-06-04T15:07:14.755295Z","iopub.status.idle":"2024-06-04T15:07:15.038557Z","shell.execute_reply.started":"2024-06-04T15:07:14.755258Z","shell.execute_reply":"2024-06-04T15:07:15.037529Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DebertaV2ForSequenceClassification(\n  (deberta): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n      (dropout): StableDropout()\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n              (pos_dropout): StableDropout()\n              (dropout): StableDropout()\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=384, out_features=384, bias=True)\n              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n              (dropout): StableDropout()\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=384, out_features=1536, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=1536, out_features=384, bias=True)\n            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n            (dropout): StableDropout()\n          )\n        )\n      )\n      (rel_embeddings): Embedding(512, 384)\n      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=384, out_features=384, bias=True)\n    (dropout): StableDropout()\n  )\n  (classifier): Linear(in_features=384, out_features=6, bias=True)\n  (dropout): StableDropout()\n)"},"metadata":{}}]},{"cell_type":"code","source":"accumulation_steps = 4  # Accumulate gradients over 4 steps\ntotal_steps = len(train_loader) * accumulation_steps\n\nfor epoch in range(1):  # Number of epochs\n    model.train()\n    optimizer.zero_grad()\n    step = 0\n\n    for batch in train_loader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device, dtype=torch.long)  # Ensure inputs are of type torch.long\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        # Perform backpropagation for gradient accumulation\n        loss = loss / accumulation_steps\n        loss.backward()\n\n        step += 1\n        if step % accumulation_steps == 0 or step == total_steps:\n            optimizer.step()\n            optimizer.zero_grad()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:07:15.039768Z","iopub.execute_input":"2024-06-04T15:07:15.04004Z","iopub.status.idle":"2024-06-04T15:27:00.951402Z","shell.execute_reply.started":"2024-06-04T15:07:15.040016Z","shell.execute_reply":"2024-06-04T15:27:00.950194Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Evaluation on test set\nmodel.eval()\ntest_preds = []\ntest_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device, dtype=torch.long)  # Ensure inputs are of type torch.long\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        preds = logits.argmax(dim=1).cpu().numpy()\n        test_preds.extend(preds)\n        test_labels.extend(labels.cpu().numpy())\n\n# Calculate Quadratic Weighted Kappa score\nqwk_score = cohen_kappa_score(test_labels, test_preds, weights='quadratic')\nprint(f\"Quadratic Weighted Kappa Score: {qwk_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:27:00.953487Z","iopub.execute_input":"2024-06-04T15:27:00.954253Z","iopub.status.idle":"2024-06-04T15:28:50.581558Z","shell.execute_reply.started":"2024-06-04T15:27:00.954195Z","shell.execute_reply":"2024-06-04T15:28:50.580508Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Quadratic Weighted Kappa Score: 0.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.save_pretrained(\"/kaggle/working/trained_deberta_model\")\nmodel.save_pretrained(\"/kaggle/working/trained_deberta_model\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T15:29:24.716705Z","iopub.execute_input":"2024-06-04T15:29:24.717591Z","iopub.status.idle":"2024-06-04T15:29:25.389545Z","shell.execute_reply.started":"2024-06-04T15:29:24.717556Z","shell.execute_reply":"2024-06-04T15:29:25.388449Z"},"trusted":true},"execution_count":16,"outputs":[]}]}